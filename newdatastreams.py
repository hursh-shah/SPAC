# -*- coding: utf-8 -*-
"""NewDataStreams.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rU_u2xH0GfuKqbJp5I_mtkkn2MRrwuNQ
"""

import numpy as np
import pandas as pd
import pandas_datareader as pddr
import datetime as dt
import os
from bokeh.io import output_file
from bokeh.plotting import figure, show
from bokeh.models import ColumnDataSource, Title, Legend, HoverTool
from bokeh.models import DatetimeTickFormatter

cur_path = os.path.split(os.path.abspath(os.getcwd()))[0]
data_fldr = 'data'
data_dir = os.path.join(cur_path, data_fldr)
data_dir

'''
Define functions
'''
def get_fedfunds_data(beg_date="earliest", end_date="most_recent",
                      download_from_internet=True):
    '''
    This function either downloads or reads in the daily frequency U.S.
    effective federal funds rate and target data series.

    Args:
        beg_date (str): either "earliest" or "yyyy-mm-dd" format date
        end_date (str): either "most_recent" or "yyyy-mm-dd" format date
        download_from_internet (bool): =True if download data from
            fred.stlouisfed.org, otherwise read data in from local directory

    Other functions and files called by this function:
        ffrates_[yyyy-mm-dd].csv

    Files created by this function:
        ffrates_[yyyy-mm-dd].csv

    Returns:
        ffrates_df (DataFrame): N x 5 DataFrame of date, ffr_effective,
            ffr_targ, ffr_targ_min, ffr_targ_max
        end_date_str2 (str): actual end date of fed funds rate time series in
            'yyyy-mm-dd' format. Can differ from the end_date input to this
            function if the final data for that day have not come out yet
            (usually 2 hours after markets close, 6:30pm EST), or if the
            end_date is one on which markets are closed (e.g. weekends and
            holidays). In this latter case, the pandas_datareader library
            chooses the most recent date for which we have fed funds rate data.
    '''
    # Warnings
    # If date is given before backstop or in the future

    if beg_date == "earliest":
        beg_date = dt.datetime.strptime("1954-07-01", '%Y-%m-%d')
    else:
        beg_date = np.maximum(dt.datetime.strptime("1954-07-01", '%Y-%m-%d'),
                              dt.datetime.strptime(beg_date, '%Y-%m-%d'))
    if end_date == "most_recent":
        end_date = dt.datetime.today()
    else:
        end_date = np.minimum(dt.datetime.today(),
                              dt.datetime.strptime(end_date, '%Y-%m-%d'))
    end_date_str = end_date.strftime('%Y-%m-%d')
    beg_date_str = beg_date.strftime('%Y-%m-%d')

    # Name the current directory and make sure it has a data folder
    cur_path = os.path.split(os.path.abspath(os.getcwd()))[0]
    data_fldr = 'data'
    data_dir = os.path.join(cur_path, data_fldr)
    if not os.access(data_dir, os.F_OK):
        os.makedirs(data_dir)

    filename_str = ('data/ffrates_' + beg_date_str + "_through_" + end_date_str + '.csv')

    if download_from_internet:
        # Download the federal funds rates data directly from
        # fred.stlouisfed.org (requires internet connection)
        ffrates_df = pddr.fred.FredReader(
            symbols=['DFF', 'DFEDTAR', 'DFEDTARL', 'DFEDTARU'],
            start=beg_date, end=end_date).read()
        ffrates_df = pd.DataFrame(ffrates_df).sort_index()  # Sort old to new
        ffrates_df = ffrates_df.reset_index(level=['DATE'])
        ffrates_df = ffrates_df.rename(columns={'DATE': 'Date',
                                                'DFF': 'ffr_effective',
                                                'DFEDTAR': 'ffr_targ',
                                                'DFEDTARL': 'ffr_targ_low',
                                                'DFEDTARU': 'ffr_targ_high'})
        end_date_str2 = ffrates_df['Date'].iloc[-1].strftime('%Y-%m-%d')
        end_date = dt.datetime.strptime(end_date_str2, '%Y-%m-%d')
        filename_str = ('data/ffrates_' + beg_date_str + "_through_" + end_date_str + '.csv')
        ffrates_df.to_csv(filename_str, index=False)
    else:
        # Import the data as pandas DataFrame
        end_date_str2 = end_date_str
        data_file_path = os.path.join(cur_path, filename_str)
        ffrates_df = pd.read_csv(
            data_file_path,
            names=['Date', 'ffr_effective', 'ffr_targ', 'ffr_targ_low',
                   'ffr_targ_high'],
            parse_dates=['Date'], skiprows=1,
            na_values=['.', 'na', 'NaN']
        )
        # usempl_df = usempl_df.dropna()

    print('End date of U.S. federal funds rate series is',
          end_date.strftime('%Y-%m-%d'))

    return ffrates_df, end_date_str2

if not os.path.exists('data'):
    os.makedirs('data')

get_fedfunds_data(beg_date="2023-01-01", end_date="2023-08-16",
                      download_from_internet=True)

import pandas as pd

import pandas_datareader.data as web
import datetime

start = datetime.datetime(2002, 1, 1)

## CBOE Volatility Index: VIX
vix = web.DataReader("VIXCLS", "fred", start)
print(len(vix))
print(vix.tail())
print('\n')
vix = vix.reset_index()
vix = vix.loc[vix['VIXCLS'].isna() == False]

vix

# Save to CSV
vix.to_csv('data/vix_2002-01-01.csv', index=False)

vix.loc[vix['VIXCLS'].isna()]

# -*- coding: utf-8 -*-
"""

SEC Filing Scraper
@author: AdamGetbags

"""

# import modules
import requests
import pandas as pd

# create request header
headers = {'User-Agent': "email@address.com"}

# get all companies data
companyTickers = requests.get(
    "https://www.sec.gov/files/company_tickers.json",
    headers=headers
    )

# review response / keys
print(companyTickers.json().keys())

# format response to dictionary and get first key/value
firstEntry = companyTickers.json()['0']

# parse CIK // without leading zeros
directCik = companyTickers.json()['0']['cik_str']

# dictionary to dataframe
companyData = pd.DataFrame.from_dict(companyTickers.json(),
                                     orient='index')

# add leading zeros to CIK
companyData['cik_str'] = companyData['cik_str'].astype(
                           str).str.zfill(10)

# review data
print(companyData[:1])

cik = companyData[0:1].cik_str[0]

# get company specific filing metadata
filingMetadata = requests.get(
    f'https://data.sec.gov/submissions/CIK{cik}.json',
    headers=headers
    )

# review json
print(filingMetadata.json().keys())
filingMetadata.json()['filings']
filingMetadata.json()['filings'].keys()
filingMetadata.json()['filings']['recent']
filingMetadata.json()['filings']['recent'].keys()

# dictionary to dataframe
allForms = pd.DataFrame.from_dict(
             filingMetadata.json()['filings']['recent']
             )

# review columns
allForms.columns
allForms[['accessionNumber', 'reportDate', 'form']].head(50)

# 10-Q metadata
allForms.iloc[11]

# get company facts data
companyFacts = requests.get(
    f'https://data.sec.gov/api/xbrl/companyfacts/CIK{cik}.json',
    headers=headers
    )

#review data
companyFacts.json().keys()
companyFacts.json()['facts']
companyFacts.json()['facts'].keys()

# filing metadata
companyFacts.json()['facts']['dei'][
    'EntityCommonStockSharesOutstanding']
companyFacts.json()['facts']['dei'][
    'EntityCommonStockSharesOutstanding'].keys()
companyFacts.json()['facts']['dei'][
    'EntityCommonStockSharesOutstanding']['units']
companyFacts.json()['facts']['dei'][
    'EntityCommonStockSharesOutstanding']['units']['shares']
companyFacts.json()['facts']['dei'][
    'EntityCommonStockSharesOutstanding']['units']['shares'][0]

# concept data // financial statement line items
companyFacts.json()['facts']['us-gaap']
companyFacts.json()['facts']['us-gaap'].keys()

# different amounts of data available per concept
companyFacts.json()['facts']['us-gaap']['AccountsPayable']
companyFacts.json()['facts']['us-gaap']['Revenues']
companyFacts.json()['facts']['us-gaap']['Assets']

# get company concept data
companyConcept = requests.get(
    (
    f'https://data.sec.gov/api/xbrl/companyconcept/CIK{cik}'
     f'/us-gaap/Assets.json'
    ),
    headers=headers
    )

# review data
companyConcept.json().keys()
companyConcept.json()['units']
companyConcept.json()['units'].keys()
companyConcept.json()['units']['USD']
companyConcept.json()['units']['USD'][0]

# parse assets from single filing
companyConcept.json()['units']['USD'][0]['val']

# get all filings data
assetsData = pd.DataFrame.from_dict((
               companyConcept.json()['units']['USD']))

# review data
assetsData.columns
assetsData.form

# get assets from 10Q forms and reset index
assets10Q = assetsData[assetsData.form == '10-Q']
assets10Q = assets10Q.reset_index(drop=True)

# plot
assets10Q.plot(x='end', y='val')

4 * 1e11

"""

SEC Filing Scraper for SPACs

"""

import requests
import pandas as pd

def get_spac_filings(cik):
    filingMetadata = requests.get(
        f'https://data.sec.gov/submissions/CIK{cik}.json',
        headers=headers
    )

    allForms = pd.DataFrame.from_dict(filingMetadata.json()['filings']['recent'])

    spacFilings = allForms[allForms['form'].isin(spac_forms)]
    return spacFilings[['accessionNumber', 'reportDate', 'form']]

headers = {'User-Agent': "email@address.com"}
spac_forms = ['S-1', '425', 'DEFA14A', '10-K', '8-K']

companyTickers = requests.get(
    "https://www.sec.gov/files/company_tickers.json",
    headers=headers
)

companyData = pd.DataFrame.from_dict(companyTickers.json(), orient='index')
companyData['cik_str'] = companyData['cik_str'].astype(str).str.zfill(10)

all_spac_filings = pd.DataFrame()

for idx, row in companyData.head(10).iterrows():
    cik = row['cik_str']
    company_name = row['title']
    ticker = row['ticker']

    spacFilings = get_spac_filings(cik)
    spacFilings['Company'] = company_name
    spacFilings['Ticker'] = ticker

    all_spac_filings = pd.concat([all_spac_filings, spacFilings])

print(all_spac_filings)

spac_list = companyData.loc[companyData['title'].str.contains("Acquisition")]
spac_list

#import re

#str_matches = companyData.title.apply(lambda x: re.search('Acquisition', x))
#res = [i for i in range(len(str_matches)) if str_matches[i] == None]
#all_spac_filings.iloc[res, :]

!pip install feedparser

spac_list['cik_str']

url_list = ['https://www.sec.gov/edgar/browse/?CIK=51434&owner=exclude', ]
for cik in spac_list['cik_str']:
  cik = int(cik)
  new_link = "https://www.sec.gov/edgar/browse/?CIK=" + str(cik) + "&owner=exclude"
  url_list.append(new_link)

url_list

from bs4 import BeautifulSoup

for unique_url in url_list:
  url = unique_url
  headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:109.0) Gecko/20100101 Firefox/113.0'}
  soup = BeautifulSoup(requests.get(url, headers=headers).content, 'html.parser')

  for link in soup.select('[href*="data"]'):
      print(link.text)
      print(url.rsplit('/', maxsplit=1)[0] + '/' + link['href'])
      print()

import requests
import feedparser

SEC_RSS_URL = 'https://www.sec.gov/edgar/browse/?CIK=1850262&owner=exclude'

response = requests.get(SEC_RSS_URL)

feed = feedparser.parse(response.content)

#spac_entries = [entry for entry in feed.entries if 'SPAC' in entry.title or 'SPAC' in entry.summary]

#for entry in spac_entries:
#    print("Title:", entry.title)
#    print("Link:", entry.link)
#    print("Summary:", entry.summary)
#    print("-" * 80)

SEC_RSS_URL = 'https://www.sec.gov/edgar/browse/?CIK=1850262&owner=exclude'
response = requests.get(SEC_RSS_URL)
feed = feedparser.parse(response.content)

from bs4 import BeautifulSoup
from urllib.request import Request, urlopen
import re

req = Request('https://www.sec.gov/edgar/browse/?CIK=1850262&owner=exclude')
html_page = urlopen(req)

soup = BeautifulSoup(html_page, "lxml")

links = []
for link in soup.findAll('a'):
    links.append(link.get('href'))

print(links)

url = 'https://www.sec.gov/edgar/browse/?CIK=1876714&owner=exclude'
headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:109.0) Gecko/20100101 Firefox/113.0'}
soup = BeautifulSoup(requests.get(url, headers=headers).content, 'html.parser')

for link in soup.select('[href*="data"]'):
    print(link.text)
    print(url.rsplit('/', maxsplit=1)[0] + '/' + link['href'])
    print()

soup.select('[href*="data"]')

X

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import LabelEncoder
import pandas as pd

all_spac_filings = pd.DataFrame({
    'accessionNumber': [1, 2, 3, 4, 5],
    'reportDate': ['2021-01-01', '2021-02-01', '2021-03-01', '2021-04-01', '2021-05-01'],
    'form': ['10-K', 'S-1', '10-K', '425', 'DEFA14A'],
    'Market_Rise': [1, 0, 1, 0, 1]
})

le = LabelEncoder()
all_spac_filings['form_encoded'] = le.fit_transform(all_spac_filings['form'])

X = all_spac_filings[['form_encoded']]
y = all_spac_filings['Market_Rise']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = LogisticRegression()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

print("Model Accuracy: ", accuracy_score(y_test, y_pred))

print("Feature importances: ", model.coef_)

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
import pandas as pd

all_spac_filings['reportDate'] = pd.to_datetime(all_spac_filings['reportDate'])

all_spac_filings['year'] = all_spac_filings['reportDate'].dt.year
feature_data = pd.get_dummies(all_spac_filings[['year', 'Company', 'form']], columns=['form'])
feature_summary = feature_data.groupby(['year', 'Company']).sum().reset_index()

vix_data = pd.read_csv('/content/data/vix_2002-01-01.csv')
vix_data['Date'] = pd.to_datetime(vix_data['Date'])
vix_data['year'] = vix_data['Date'].dt.year
vix_yearly = vix_data.groupby('year')['VIX_Close'].mean().reset_index()
vix_yearly['Market_Rise'] = (vix_yearly['VIX_Close'].diff() < 0).astype(int)

merged_data = pd.merge(feature_summary, vix_yearly, on='year', how='inner')

X = merged_data.drop(['year', 'Company', 'VIX_Close', 'Market_Rise'], axis=1)
y = merged_data['Market_Rise']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = RandomForestClassifier()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

print("Model Accuracy: ", accuracy_score(y_test, y_pred))
print("Feature Importances: ", model.feature_importances_)