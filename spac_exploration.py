# -*- coding: utf-8 -*-
"""SPAC Exploration.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16mUv_C7d8_s8aTqkyyWxncmfazp1sWBG
"""

from datetime import datetime
from typing import List


class Filing(object):

    def __init__(self, filing_type: str, url: str,
                 filing_date: datetime.date, accepted_date: datetime.date,
                 period_of_report: str, documents: List[str]):
        """Initialize filing."""
        self.filing_type = filing_type
        self.url = url
        self.filing_date = filing_date
        self.accepted_date = accepted_date
        self.period_of_report = period_of_report,
        self.documents = documents

from datetime import datetime
from lxml import html
from typing import List
import re
import requests
import lxml


BASE_URL = "https://www.sec.gov"
FILING_TYPES = ["8-K", "10-K"]


def get_request(url: str, timeout: int) -> lxml.html.HtmlElement:
    """Send request to server and output response in HtmlElement."""
    page = requests.get(url, timeout=timeout)
    return html.fromstring(page.content)


def extract_date(text: str) -> str:
    """Extract date from text."""
    match = re.search(r'\d{4}-\d{2}-\d{2}', text)
    return match.group()


def extract_date_time(text: str) -> str:
    """Extract date time from text."""
    match = re.search(r'\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}', text)
    return match.group()


class Company(object):

    def __init__(self, name, cik, timeout=10):
        """Initialize company object, for pulling documents from SEC."""
        self.name = name
        self.cik = cik
        self.url = ("https://www.sec.gov/cgi-bin/browse-edgar?action="
                    "getcompany&CIK=%s" % cik)
        self.timeout = timeout

    def _get(self, url: str) -> requests.Response:
        """Sends get request to given url for response object.

        Given url, query url server with an HTTP request.
        Args:
            url: String url of given server.
        Returns:
            request.Response object which contains the server's response to an
            HTTP request.
        """
        return requests.get(url, timeout=self.timeout)

    def get_filings_url(self, filing_type: str, prior_to="",
                        ownership="include", no_of_entries=100) -> str:
        """Get url for specific type of SEC filing related to company

        Given parameters construct url associated with the constraints for
        the SEC web page which contains all admissible documents.
        Args:
            filing_type: String value for document filing type.
            prior_to: Date constraint on documents.
            ownership: TODO: figure out what this means.
            no_of_entries: Number of documents to show on the webpage.
        Returns:
            String value for the url.
        """
        if filing_type not in FILING_TYPES:
            raise ValueError("not accepted filing type %s" % filing_type)
        url = (self.url + "&type=" + filing_type + "&dateb=" + prior_to +
               "&owner=" + ownership + "&count=" + str(no_of_entries))
        return url

    def get_filings_page(self, filing_type: str, prior_to="",
                         ownership="include",
                         no_of_entries=100) -> lxml.html.HtmlElement:
        """Get HtmlElement object associated w/ SEC query.

        Take page content and create an HtmlElement object. Example web page
        here https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK=
        0001741231&type=8-K&dateb=&owner=include&count=40&search_text=
        Args:
            filing_type: String value for document filing type.
            prior_to: Date constraint on documents.
            ownership: TODO: figure out what this means.
            no_of_entries: Number of documents to show on the webpage.
        Returns:
            HtmlElement representing SEC documents webpage.
        """
        url = self.get_filings_url(filing_type, prior_to, ownership,
                                   no_of_entries)
        page = self._get(url)
        return html.fromstring(page.content)

    def get_all_filings(self, filing_type: str, prior_to="",
                        ownership="include",
                        no_of_documents=100) -> List[Filing]:
        """Get all filings of certain type.

        Using the url, aggregate the documents for each filing, and all other
        relevant data.
        Args:
            filing_type: String value for document filing type.
            prior_to: Date constraint on documents.
            ownership: TODO: figure out what this means.
            no_of_documents: Number of documents to show on the webpage.
        Returns:
            List of filing objects, each of which contains text of relevant
            documents. TODO: add all the documents, currently only uses 8-K
        """
        filings_page = self.get_filings_page(
            filing_type=filing_type,
            prior_to=prior_to,
            ownership=ownership,
            no_of_entries=no_of_documents
        )
        elems = filings_page.xpath(
            '//*[@id="documentsbutton"]')[:no_of_documents]
        all_filings = []
        for elem in elems:
            filing_url = BASE_URL + elem.attrib["href"]
            filing_page = get_request(filing_url, self.timeout)
            filing_page_content = filing_page.find_class("formContent")[
                    0].text_content()

            # Get the relevant dates.
            filing_date = extract_date(
                re.search("Filing Date\n(.*)\n",
                          filing_page_content).group(1))
            filing_date = datetime.strptime(
                filing_date, '%Y-%m-%d')

            accepted_date = extract_date_time(
                re.search("Accepted\n(.*)\n",
                          filing_page_content).group(1))
            accepted_date = datetime.strptime(
                accepted_date, '%Y-%m-%d %H:%M:%S')

            period_of_report = extract_date(
                re.search("Period of Report\n(.*)\n",
                          filing_page_content).group(1))

            # Extract text from documents in filing.
            document_url = (BASE_URL + filing_page.xpath(
                '//*[@id="formDiv"]/div/table/tr[2]/td[3]/a')[0].attrib[
                "href"]).replace('/ix?doc=', '')
            document = get_request(document_url, self.timeout)

            # Construct filing object.
            filing = Filing(
                filing_type=filing_type,
                url=filing_url,
                filing_date=filing_date,
                accepted_date=accepted_date,
                period_of_report=period_of_report,
                documents=[document.text_content()]
            )
            all_filings.append(filing)
        return all_filings

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd


URL_SEC_MAPPING = 'https://www.sec.gov/files/company_tickers.json'

# TODO: add "U" and "-UN" symbol parsing


def load_sec_mappings() -> pd.DataFrame:
    """Load SEC mapping information.

    Load in JSON file mapping cik id to ticker to company name. Transforms
    all tickers to upper case, and all data is stored in pandas dataframe.
    """
    # Load in SEC mapping of cik id, ticker, and title.
    sec_mapping = pd.read_json(URL_SEC_MAPPING).transpose()
    sec_mapping.ticker = sec_mapping.ticker.str.upper()
    sec_mapping.cik_str = sec_mapping.cik_str.astype(str)
    return sec_mapping


class SEC(object):

    def __init__(self):
        """Initialize function for SEC data."""
        self.sec_mapping = load_sec_mappings()

    def get_name_by_cik(self, cik: str) -> str:
        """Get company name from cik.

        Given cik use SEC company tickers json file to find the corresponding
        company name.
        Args:
            cik: String value of cik.
        Return:
            String value of company name.
        """
        if cik not in self.sec_mapping.cik_str.to_list():
            raise ValueError("cik %s not found in SEC mapping %s"
#                              % (cik, URL_SEC_MAPPING))
        else:
            return (self.sec_mapping[self.sec_mapping.cik_str == cik].
                    title.to_list()[0])

    def get_cik_by_name(self, name: str) -> str:
        """Get cik from company name.

        Given company name use SEC company tickers json file to find the
        corresponding cik.
        Args:
            name: String value of company name.
        Return:
            String value of cik.
        """
        if name not in self.sec_mapping.title.to_list():
            raise ValueError("name %s not found in SEC mapping %s"
#                              % (name, URL_SEC_MAPPING))
        else:
            return (self.sec_mapping[self.sec_mapping.title == name].
                    cik_str.to_list()[0])

    def get_ticker_by_cik(self, cik: str) -> str:
        """Get company ticker from cik.

        Given cik use SEC company tickers json file to find the corresponding
        company ticker.
        Args:
            cik: String value of cik.
        Return:
            String value of ticker.
        """
        if cik not in self.sec_mapping.cik_str.to_list():
            raise ValueError("cik %s not found in SEC mapping %s"
#                              % (cik, URL_SEC_MAPPING))
        else:
            return (self.sec_mapping[self.sec_mapping.cik_str == cik].
                    ticker.to_list()[0])

    def get_cik_by_ticker(self, ticker: str) -> str:
        """Get cik from ticker.

        Given ticker use SEC company tickers json file to find the
        corresponding cik.
        Args:
            ticker: String value of company name.
        Return:
            String value of ticker.
        """
        if ticker not in self.sec_mapping.ticker.to_list():
            raise ValueError("ticker %s not found in SEC mapping %s"
#                              % (ticker, URL_SEC_MAPPING))
        else:
            return (self.sec_mapping[self.sec_mapping.ticker == ticker].
                    cik_str.to_list()[0])

    def get_name_by_ticker(self, ticker: str) -> str:
        """Get company from ticker.

        Given ticker use SEC company tickers json file to find the
        corresponding company name.
        Args:
            ticker: String value of company name.
        Return:
            String value of company name.
        """
        return self.get_name_by_cik(self.get_cik_by_ticker(ticker))

import os
import pandas as pd

BASE_PATH = "https://github.com/alandu20/spac/tree/master/data/"


def get_old_spac_tickers(file_path):
    """Get list of old spac tickers."""
    # Past spac list (completed business combination).
    spac_list_past = pd.read_csv(file_path)
    spac_list_past.fillna('missing', inplace=True)
    spac_list_past['dupe_filter'] = spac_list_past['Old Ticker'] + \
                                    spac_list_past['New Ticker']
    spac_list_past = spac_list_past[
        spac_list_past.dupe_filter.isin(spac_list_past.dupe_filter.unique())]
    spac_list_past.drop(columns=['dupe_filter'], inplace=True)
    return spac_list_past['Old Ticker'].to_list()


def get_current_spac_tickers(file_path):
    """Get list of new spac tickers."""
    # Current spac list.
    spac_list_current = pd.read_csv(file_path)
    spac_list_current = spac_list_current.Ticker.unique()
    spac_list_current = pd.DataFrame(spac_list_current, columns=['Ticker'])
    return spac_list_current['Ticker'].to_list()


def main():
    # Instantiate SEC map.
    sec_map = SEC()

    # File paths.
    file_path_current = "https://raw.githubusercontent.com/alandu20/spac/master/data/spac_list_current.csv"
    file_path_past = "https://raw.githubusercontent.com/alandu20/spac/master/data/spac_list_past.csv"

    for spac in get_old_spac_tickers(file_path_past):
        print(spac)
        try:
            company_name = sec_map.get_name_by_ticker(spac)
            cik = sec_map.get_cik_by_ticker(spac)
        except ValueError:
            continue

        # Track file path for company
        file_path_company = BASE_PATH + "/%s" % spac

        # Get filings.
        c = Company(company_name, cik)
        filings = c.get_all_filings("8-K")

        # Dump out text data.
        for filing in filings:
            file_path_filing = file_path_company + "/%s" % filing.accepted_date
            if not os.path.exists(file_path_filing):
                os.makedirs(file_path_filing)
            file_path_document = file_path_filing + "/%s" % "8-K.txt"
            with open(file_path_document, "w") as doc:
                doc.write(filing.documents[0])

file_path_current = 'https://raw.githubusercontent.com/alandu20/spac/master/data/spac_list_current.csv'
spac_list_current = pd.read_csv(file_path_current)
spac_list_current = spac_list_current.Ticker.unique()
spac_list_current = pd.DataFrame(spac_list_current, columns=['Ticker'])
spac_list_current['Ticker'].to_list()[0:5]

spac_stat = pd.read_csv('SPAC Statistics.csv')

spac_stat

spac_stat2 = pd.read_csv('SPAC Statistics (1).csv')

spac_stat2

import seaborn as sns
import matplotlib.pyplot as plt

spac_stat.describe()

spac_stat2.describe()

spac_stat.corr()

spac_stat2.corr()

sns.heatmap(spac_stat.corr())

sns.heatmap(spac_stat2.corr())

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

spac_stat2['Ipo Date'] = pd.to_datetime(spac_stat2['Ipo Date'])
spac_stat2['Gross Public Proceeds ($mm)'] = spac_stat2['Gross Public Proceeds ($mm)'].replace(',', '', regex=True).astype(float)

summary_statistics = spac_stat2.describe()

spac_stat2.drop(columns=['Ipo Date']).hist(figsize=(15, 7), bins=10, grid=False, color='skyblue', edgecolor='black')
plt.suptitle('Distribution of SPAC IPO Statistics')
plt.show()

plt.figure(figsize=(15, 10))
for column in spac_stat2.columns[1:]:
    plt.plot(spac_stat2['Ipo Date'], spac_stat2[column], marker='o', label=column)
plt.title('Trends in SPAC IPO Statistics Over Time')
plt.xlabel('IPO Date')
plt.ylabel('Value')
plt.legend()
plt.grid(True)
plt.show()

correlation_matrix = spac_stat2.drop(columns=['Ipo Date']).corr()
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=.5)
plt.title('Correlation Matrix of SPAC IPO Statistics')
plt.show()

spac_stat.drop(columns=['Year']).hist(figsize=(15, 10), bins=10, grid=False, color='skyblue', edgecolor='black')
plt.suptitle('Distribution of SPAC Statistics')
plt.show()

plt.figure(figsize=(15, 10))
for column in spac_stat.columns[1:]:
    plt.plot(spac_stat['Year'], spac_stat[column], marker='o', label=column)
plt.title('Trends in SPAC Statistics Over Time')
plt.xlabel('Year')
plt.ylabel('Count')
plt.legend()
plt.grid(True)
plt.show()

correlation_matrix = spac_stat.drop(columns=['Year']).corr()
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=.5)
plt.title('Correlation Matrix of SPAC Statistics')
plt.show()

!pip install yfinance
!pip install yahoo_fin